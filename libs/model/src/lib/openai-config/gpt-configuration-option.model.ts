
/**
 * In this example, we have defined an interface called ConfigurationOptions that describes the shape of the configuration options for the AI writer app.

The interface includes a model property of type string, representing the selected OpenAI model.

The pricePerToken property is a number value representing the cost per token generated by the model.

The completionOptions property is an object with properties for maxTokens, temperature, topP, presencePenalty, and frequencyPenalty, each of which is of type number or boolean, representing the selected completion options.

The additionalOptions property is an object with properties for saveGeneratedText and showModelInfo. The saveGeneratedText property is a boolean value indicating whether the user has enabled the option to save the generated text to a file. The showModelInfo property is a function that displays information about the selected model when called.

This interface can be used to ensure that the configuration options for the AI writer app are of the correct type and structure, which can help prevent errors and improve the user experience.
 */
export interface GPTConfigurationOptions {
  model: string;
  pricePerToken: number;
  completionOptions: CompletionOptions;
  additionalOptions: {
    saveGeneratedText: boolean;
    showModelInfo: string;
  };
}



/**
 * name - A string representing the name of the ChatGPT model.
description - A string representing a brief description of the ChatGPT model.
id - A string representing the unique identifier of the ChatGPT model.
 */
export interface GPTModelName {
  name: string;
  description: string;
  id: string;
}

/**
 * Davinci - Represents the Davinci model.
Curie - Represents the Curie model.
Babbage - Represents the Babbage model.
Ada - Represents the Ada model.
GPT3 - Represents the GPT-3 model, also known as the Davinci-002 model.
 */
export enum ChatGPTModel {
  Davinci = "davinci",
  Curie = "curie",
  Babbage = "babbage",
  Ada = "ada",
  GPT3 = "davinci-002",
}


/**
 * maxTokens - A number representing the maximum number of tokens that should be generated in the response.
temperature - A number representing the randomness of the generated response.
topP - A number representing the top-p probability mass to use for nucleus sampling.
presencePenalty - A boolean indicating whether to penalize new tokens based on their presence in the prompt.
frequencyPenalty - A boolean indicating whether to penalize new tokens based on their frequency in the generated response so far.
 */
export interface CompletionOptions {
  maxTokens: number;
  temperature: number;
  topP: number;
  presencePenalty: boolean;
  frequencyPenalty: boolean;
}

/**
 * 
saveGeneratedText - A boolean indicating whether to save the generated text to a file.
showModelInfo - A string representing which model information should be displayed.
 */
export interface AdditionalOptions {
  saveGeneratedText: boolean;
  showModelInfo: string;
}
